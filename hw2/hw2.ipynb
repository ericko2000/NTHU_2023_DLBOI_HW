{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li0bVCTuxc6n"
   },
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# National Tsing Hua University\n",
    "\n",
    "### Fall 2023\n",
    "\n",
    "#### 11210IPT 553000\n",
    "\n",
    "#### Deep Learning in Biomedical Optical Imaging\n",
    "\n",
    "## Homework 2\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ✏️ Task A: Transitioning to Cross-Entropy Loss (20 pts)\n",
    "\n",
    "In Lab, we utilized the **Binary Cross-Entropy (BCE) Loss** for a binary classification task. The BCE loss is articulated as:\n",
    "\n",
    "$$ \\text{BCE}(y, \\hat{y}) = - \\left( y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right) $$\n",
    "\n",
    "Here, $y$ is the true label (0 or 1), and $\\hat{y}$ denotes the predicted probability of $y=1$. \n",
    "\n",
    "In this task, we aim to explore the implementation of a model using **Cross-Entropy (CE) Loss**, which is a more common approach for classification tasks, especially when dealing with multiple classes. CE loss is expressed as:\n",
    "\n",
    "$$ \\text{CE}(y, \\hat{y}) = -\\sum_{i} y^{(i)} \\log(\\hat{y}^{(i)}) $$\n",
    "\n",
    "In this expression, $y$ represents the ground truth labels, $ \\hat{y} $ is the predictions from your model, and $i$ is the index of the class.\n",
    "\n",
    "\n",
    "#### 1. Modify the Loss (3 pts)\n",
    "Transition to using Cross-Entropy (CE) Loss for the classification task by utilizing PyTorch's built-in functionalities. You can refer to the [official PyTorch documentation](https://pytorch.org/docs/stable/nn.html) for detailed information and guidance to ensure the correct implementation of the CE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Code by 111003804\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# Replace '...' with the appropriate loss function in PyTorch\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Modify the Model Architecture (2 pts)\n",
    "To adapt the original code for use with Cross-Entropy (CE) loss, make necessary modifications to the model architecture. Ensure it is compatible and optimized for the application of CE loss. Consider the number of output nodes and the activation function used in the output layer for effective multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Code by 111003804\n",
    "\n",
    "# Modifying the architecture to be compatible with CE loss\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(256*256*1, 256),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    # Modification 2\n",
    "    # The output dimension should be 2, corresponding to 2 classes in the cross-entropy loss.\n",
    "    # nn.Linear(256, 1) # original\n",
    "    nn.Linear(256, 2)  # new\n",
    "    \n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3. Reflection Questions (15 pts, 5 pts for each)\n",
    "Provide detailed answers to the questions below:\n",
    "\n",
    "**Q1. Loss Function Comparison:**  \n",
    "   What are the differences between Binary Cross-Entropy (BCE) loss and Cross-Entropy (CE) loss?\n",
    "\n",
    "**Q2. Model Architecture Modification:**  \n",
    "   What motivated the specific changes you made to the model architecture?\n",
    "\n",
    "**Q3. Adapting to CE Loss:**  \n",
    "   In the original code configured for BCE loss, two major adjustments are needed for adaptation to CE loss. Analyze and explain the necessity for these changes, referring to the code below.\n",
    "\n",
    "```python\n",
    "for images, labels in train_loader:\n",
    "    images = images.cuda()\n",
    "    images = images / 255.0\n",
    "    labels = labels.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images)\n",
    "\n",
    "    # Change #1: Adaptation to the labels for CE loss\n",
    "    labels = labels.long()  # Changed from labels.float().unsqueeze(1) for BCE loss\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    # Change #2: Predictions for CE loss\n",
    "    train_predicted = outputs.argmax(-1)  # Changed from torch.sigmoid(outputs) > 0.5 for BCE loss\n",
    "    train_correct += (train_predicted == labels).sum().item()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put Your Response Here:\n",
    "\n",
    "##### 1. \n",
    "Here are the key differences between these two loss functions:\n",
    "\n",
    "##### a. Number of Classes:\n",
    "\n",
    "Binary Cross-Entropy (BCE) Loss: BCE loss is used when you have a **binary classification problem**, meaning there are only two classes (e.g., classifying images as \"cat\" or \"not cat\").\n",
    "\n",
    "Cross-Entropy (CE) Loss: CE loss is used for **multi-class classification problems**, where there are usually more than two classes (e.g., classifying images into multiple categories like \"cat,\" \"dog,\" \"bird,\" etc.). However, we can also apply CE Loss to binary classification problems, as in this case.\n",
    "\n",
    "\n",
    "##### b. Output Activation Function:\n",
    "\n",
    "Binary Cross-Entropy (BCE) Loss: In binary classification, the final layer of the neural network typically uses a **sigmoid activation function**, and BCE loss is used with this sigmoid output to measure the difference between predicted probabilities and actual labels.\n",
    "\n",
    "Cross-Entropy (CE) Loss: For multi-class classification, the final layer often uses a **softmax activation function** to produce class probabilities, and CE loss is applied to compare the predicted probabilities to the true class labels.\n",
    "\n",
    "\n",
    "##### c. Loss Formulation:\n",
    "\n",
    "Binary Cross-Entropy (BCE) Loss: BCE loss is often formulated as the negative log-likelihood loss for binary classification. It encourages the predicted probability of the positive class to be close to 1 and the predicted probability of the negative class to be close to 0 for each example.\n",
    "\n",
    "$$Binary\\ Cross-Entropy\\ (BCE)\\ Loss=\\frac{1}{N} \\sum_{i=1}^{N} \\ -[ \\ y_i \\ log(\\hat y_i) \\ + \\ (1 - y_i) \\ log(1 - \\hat y_i)\\ ] $$\n",
    "\n",
    "Cross-Entropy (CE) Loss: CE loss is also a negative log-likelihood loss, but it considers multiple classes. It encourages the predicted probability of the correct class to be close to 1 and the probabilities of other classes to be close to 0 for each example.\n",
    "$$Cross-Entropy\\ (CE)\\ Loss=\\frac{1}{N} \\sum_{i=1}^{N} \\ \\ \\sum_{j=1}^{C} \\ - \\ y_{i, j} \\ log(\\hat y_{i, j})  $$\n",
    "\n",
    "\n",
    "##### d. Output Format:\n",
    "\n",
    "Binary Cross-Entropy (BCE) Loss: The output of a binary classification model is a single value (probability) per example, which represents the likelihood of belonging to the positive class.\n",
    "\n",
    "Cross-Entropy (CE) Loss: The output of a multi-class classification model is a vector of class probabilities (one probability per class) per example, and CE loss compares the entire probability distribution with the true labels.\n",
    "\n",
    "\n",
    "##### e. Loss Calculation:\n",
    "\n",
    "Binary Cross-Entropy (BCE) Loss: BCE loss is computed independently for each binary classification example and then averaged across all examples.\n",
    "\n",
    "Cross-Entropy (CE) Loss: CE loss considers all classes for each multi-class classification example and is calculated as the negative logarithm of the predicted probability of the true class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. \n",
    "To apply CE loss to a binary classification task, several modifications are required. Initially, binary labels (0 or 1) are transformed into two-dimensional one-hot vectors, where each vector has two components: one for the true class and the other for the opposite class. This is akin to a simplified version of the one-hot encoding used in multi-class classification problems.\n",
    "\n",
    "Subsequently, the model's architecture must be adapted to produce a two-dimensional output. This entails configuring the output layer with two neurons, with each neuron representing one of the binary classes. The outputs are passed through a softmax activation function to obtain class probabilities, indicating the likelihood of each class being the correct class for a given example.\n",
    "\n",
    "The need for this architectural change is rooted in the requirements of CE loss, which is designed for multi-class classification problems. If the model's architecture remains unchanged with a one-dimensional output, it will result in an error when calculating the loss. This error arises from the mismatch between the loss function's expectation of a two-class output and the model's single-dimensional output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.\n",
    "\n",
    "##### a. Adaptation to Label Format:\n",
    "\n",
    "In the original code designed for BCE loss, labels were represented as float values between 0 and 1, suitable for a binary classification problem where BCE loss is used. Each label indicated the likelihood of the positive class (class 1).\n",
    "\n",
    "For CE loss, the labels have to be converted into a format suitable for multi-class classification, even though it is still a binary classification problem. Labels should be one-hot encoded or represented as integers. In the adapted code, labels = labels.long() is used to convert labels to integer format. However, in my own code, I don't need to convert the label's data type here because I do that during the data preprocessing phase. Actually, I think we need to use labels = labels.float() instead of labels.long() because the EC loss is calculated using float .\n",
    "\n",
    "This adaptation is necessary because CE loss expects integer labels (0 or 1) that correspond to the class index. When using BCE loss, the sigmoid function is employed to handle float labels, but in CE loss, labels should be integers to indicate the true class directly.\n",
    "\n",
    "\n",
    "##### b. Change in Predictions for CE Loss:\n",
    "\n",
    "In the original BCE loss configuration, predictions were obtained by applying a sigmoid activation function to the model's outputs and then thresholding the results with > 0.5. This approach produces binary predictions (0 or 1) for the positive class based on the output's likelihood.\n",
    "\n",
    "In the context of CE loss, predictions should be based on the class with the highest predicted probability. Therefore, the adapted code uses train_predicted = outputs.argmax(-1) to find the class index with the highest probability for each example. But, in my own code, I use train_predicted = torch.argmax(outputs, dim=1) instead.\n",
    "\n",
    "This change ensures that the predictions are suitable for CE loss, where each example can be assigned to one of the two classes based on the maximum probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✏️ Task B: Creating an Evaluation Code (20 pts)\n",
    "\n",
    "Evaluate the performance of a pretrained deep learning model with a test dataset of chest X-ray images available in `test_normal.npy` and `test_pneumonia.npy` files. These files respectively contain 200 grayscale normal and pneumonia chest X-ray images, each of size 256×256. The objective is to calculate the model’s accuracy, defined as the percentage of images correctly classified. To accomplish this, you are tasked to write code that loads, processes, and evaluates the model on this specific dataset. Ensure each segment of code replacing the `...` placeholders is functional and aligns with the steps provided in the instructions.\n",
    "\n",
    "**Note: ⚠️ Ensure to upload your trained model's weights to your working environment if needed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Download test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/TacoXDD/homeworks/master/dataset/test/test_normal.npy\n",
    "!wget https://raw.githubusercontent.com/TacoXDD/homeworks/master/dataset/test/test_pneumonia.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare your test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_abnormal = np.load('test_pneumonia.npy')\n",
    "test_normal = np.load('test_normal.npy')\n",
    "\n",
    "print(f'Shape of test_abnormal: {test_abnormal.shape}')\n",
    "print(f'Shape of test_normal: {test_normal.shape}')\n",
    "\n",
    "# For the data having presence of pneumonia assign 1, for the normal ones assign 0.\n",
    "test_abnormal_labels = np.ones((test_abnormal.shape[0],))\n",
    "test_normal_labels = np.zeros((test_normal.shape[0],))\n",
    "\n",
    "x_test = np.concatenate((test_abnormal, test_normal), axis=0)\n",
    "y_test = np.concatenate((test_abnormal_labels, test_normal_labels), axis=0)\n",
    "\n",
    "print(f'Shape of x_test: {x_test.shape}')\n",
    "print(f'Shape of y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load Test Images into PyTorch DataLoader (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_test = torch.from_numpy(x_test).to(torch.float)\n",
    "y_test = torch.from_numpy(y_test).to(torch.long)\n",
    "\n",
    "# Combine the images and labels into a dataset\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "# Create a dataloader to load data in batches. Set batch size to 32.\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare Your Trained Model  (5 pts)\n",
    "- Define the architecture to match exactly with the trained model intended for inference. Ensure strict alignment to avoid errors during evaluation.\n",
    "- Load the weights from the trained model and set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the model architecture\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(256*256*1, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 1)\n",
    ").cuda()\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('model_classification_BCEL_L2_W256.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Perform Inference and Calculate the Accuracy (10 pts)\n",
    "- Ensure the image values are processed in a manner consistent with the training phase.\n",
    "- Use the model that was trained with BCE loss to execute inference on the test dataset.\n",
    "- Note that inference should be performed in GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.cuda()\n",
    "        images = images / 255.0\n",
    "        labels = labels.cuda()\n",
    "        labels = labels.to(dtype=torch.float).unsqueeze(1)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        predicted = torch.sigmoid(outputs) > 0.5\n",
    "        test_correct += (predicted.float() == labels).sum().item()\n",
    "        \n",
    "        test_total += labels.shape[0]\n",
    "\n",
    "    test_accuracy = 100. * test_correct / test_total\n",
    "        \n",
    "\n",
    "# print(f'Test Loss = {avg_test_loss}')\n",
    "print(f'Test accuracy is {test_accuracy}')        \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
